{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from itertools import product\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, Input, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_VALUE = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv('../../data/data_output.csv')\n",
    "\n",
    "with open('../../data/X_general.pkl', 'rb') as f:\n",
    "    X_general = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an ensemble for the estimation of one parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aleatoric_loss(y_true, y_pred):\n",
    "    \"\"\"Custom loss function for estimating aleatoric loss\"\"\"\n",
    "    var_est = y_pred[:, 0]\n",
    "    loss = 0.5 * (tf.math.log(var_est) + y_true / var_est)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def create_model(max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, dimension_general,\n",
    "                 special_value, lstm_units=128, dense_units=128, num_lstm_layers=1, num_dense_hidden_layers=2, \n",
    "                 transfer_learning_rate=0.001, activation='relu', transfer_learning=False, aleatoric_uncertainty=False):\n",
    "    input_layer_vector = Input(name=\"input_vector\", shape=(max_seq_len_vector, dimension_vector))\n",
    "    input_layer_scalar = Input(name=\"input_scalar\", shape=(max_seq_len_scalar, dimension_scalar))\n",
    "    input_general = Input(name=\"input_general\", shape=(dimension_general,))\n",
    " \n",
    "    masked_vector = Masking(name=\"masking_input_vector\", mask_value=special_value)(input_layer_vector)\n",
    "    masked_scalar = Masking(name=\"masking_input_scalar\", mask_value=special_value)(input_layer_scalar)\n",
    "\n",
    "    # Create LSTM layers with the specified number of units and layers\n",
    "    lstm_output_vector = masked_vector\n",
    "    lstm_output_scalar = masked_scalar\n",
    "\n",
    "    for i in range(num_lstm_layers):\n",
    "        lstm_output_vector = LSTM(lstm_units, name=f\"lstm_vector_{i}\", return_sequences=True if i < num_lstm_layers - 1 else False)(lstm_output_vector)\n",
    "        lstm_output_scalar = LSTM(lstm_units, name=f\"lstm_scalar_{i}\", return_sequences=True if i < num_lstm_layers - 1 else False)(lstm_output_scalar)\n",
    "\n",
    "    concat = concatenate([lstm_output_vector, lstm_output_scalar, input_general])\n",
    "\n",
    "    # Create dense layers with the specified number of units and layers\n",
    "    dense_output = concat\n",
    "    for i in range(num_dense_hidden_layers):\n",
    "        dense_output = Dense(dense_units, activation=activation, name=f\"dense_{i}\")(dense_output)\n",
    "\n",
    "    output = Dense(1, name='output', activation='exponential' if aleatoric_uncertainty else 'linear')(dense_output)\n",
    "\n",
    "    model = Model(inputs=[input_layer_vector, input_layer_scalar, input_general], outputs=output)\n",
    "\n",
    "    optimizer = Adam(transfer_learning_rate if transfer_learning else 0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=aleatoric_loss if aleatoric_uncertainty else 'mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an ensemble for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_transfer_learning(base_model, max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, dimension_general,\n",
    "                 special_value, lstm_units=128, dense_units=128, num_lstm_layers=1, num_dense_hidden_layers=2, \n",
    "                 transfer_learning_rate=0.001, activation='relu'):\n",
    "    new_model = create_model(max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, dimension_general,\n",
    "                             special_value, lstm_units, dense_units, num_lstm_layers, num_dense_hidden_layers, \n",
    "                             transfer_learning_rate, activation, transfer_learning=True)\n",
    "    \n",
    "    # Load pretrained weights\n",
    "    for i in range(num_lstm_layers):\n",
    "        new_model.get_layer(name=f\"lstm_vector_{i}\").set_weights(base_model.get_layer(name=f\"lstm_vector_{i}\").get_weights())\n",
    "        #new_model.get_layer(name=f\"lstm_vector_{i}\").trainable = False\n",
    "        new_model.get_layer(name=f\"lstm_scalar_{i}\").set_weights(base_model.get_layer(name=f\"lstm_scalar_{i}\").get_weights())\n",
    "        #new_model.get_layer(name=f\"lstm_scalar_{i}\").trainable = False\n",
    "\n",
    "    for i in range(num_dense_hidden_layers):\n",
    "        new_model.get_layer(name=f\"dense_{i}\").set_weights(base_model.get_layer(name=f\"dense_{i}\").get_weights())\n",
    "        # new_model.get_layer(name=f\"dense_{i}\").trainable = False\n",
    "        \n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a single model to predict SOH and concatenate it to input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Ensemble for all parameters with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X_vector, X_scalar, X_general, y):\n",
    "    n_samples = X_vector.shape[0]\n",
    "    indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    return X_vector[indices], X_scalar[indices], X_general[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ensemble(n_members, X_vector_train, X_scalar_train, X_general_train, y_train, \n",
    "                 max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, special_value,\n",
    "                 model_hyperparameter, train_hyperparameter, parameter, pretrained_model=None):\n",
    "    ensemble = list()\n",
    "    for _ in range(n_members):\n",
    "        # Generate bootstrap sample\n",
    "        X_vector_boot, X_scalar_boot, X_general_boot, y_boot = bootstrap_sample(X_vector_train, X_scalar_train, X_general_train, y_train)\n",
    "        \n",
    "        if pretrained_model is not None:\n",
    "            model = create_model_transfer_learning(pretrained_model, max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, X_general_train.shape[1], special_value, **model_hyperparameter)\n",
    "        else:\n",
    "            model = create_model(max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, X_general_train.shape[1], special_value, **model_hyperparameter)\n",
    "        \n",
    "        num_epochs = 'transfer_epochs' if pretrained_model is not None else 'normal_epochs'\n",
    "        model.fit([X_vector_boot, X_scalar_boot, X_general_boot], y_boot, epochs=train_hyperparameter[num_epochs], batch_size=train_hyperparameter['batch_size'], verbose=0)\n",
    "        \n",
    "        ensemble.append(model)\n",
    "    \n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models to estimate aleatoric uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_mean_std(ensemble, X_vector, X_scalar, X_general):\n",
    "    predictions = np.array([model.predict([X_vector, X_scalar, X_general]) for model in ensemble])\n",
    "    means = np.mean(predictions, axis=0)\n",
    "    stds = np.std(predictions, axis=0)\n",
    "    return means, stds\n",
    "\n",
    "def estimate_aleatoric_uncertainty(ensemble,\n",
    "                                   max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, dimension_general, special_value, \n",
    "                                   X_vector_test, X_scalar_test, X_general_test, y_test,\n",
    "                                   model_hyperparameter, train_hyperparameter):\n",
    "    means, stds = ensemble_mean_std(ensemble, X_vector_train, X_scalar_train, X_general_train)\n",
    "    var = np.power(stds, 2)\n",
    "    residuals_squared = np.power(y_train - means, 2)\n",
    "    r_i_squared = np.maximum(residuals_squared - var, 0)\n",
    "    model = create_model(max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, dimension_general, special_value, aleatoric_uncertainty=True, **model_hyperparameter)    \n",
    "    model.fit([X_vector_train, X_scalar_train, X_general_train], r_i_squared, epochs=train_hyperparameter['normal_epochs'], batch_size=train_hyperparameter['batch_size'], verbose=0)\n",
    "    aleatoric_var = model.predict([X_vector_test, X_scalar_test, X_general_test])\n",
    "    return np.sqrt(aleatoric_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Bootstrap Ensemble for all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_bound(lower_bound, upper_bound, min_val, max_val, z_score, std):\n",
    "    for i in range(len(lower_bound)):\n",
    "        if min_val < lower_bound[i] and max_val > upper_bound[i]:\n",
    "            continue  # Case 1\n",
    "        elif lower_bound[i] < min_val and max_val > upper_bound[i]:\n",
    "            lower_bound[i] = min_val  # Case 2\n",
    "        elif min_val < lower_bound[i] and upper_bound[i] > max_val:\n",
    "            upper_bound[i] = max_val  # Case 3\n",
    "        elif lower_bound[i] < min_val and upper_bound[i] > max_val:\n",
    "            lower_bound[i] = min_val\n",
    "            upper_bound[i] = max_val  # Case 4\n",
    "        elif lower_bound[i] > max_val:\n",
    "            upper_bound[i] = max_val\n",
    "            lower_bound[i] = max(min_val, max_val - z_score * std[i])  # Case 5\n",
    "        elif upper_bound[i] < min_val:\n",
    "            lower_bound[i] = min_val\n",
    "            upper_bound[i] = min(max_val, min_val + z_score * std[i])  # Case 6\n",
    "            \n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(ensemble, X_vector_test, X_scalar_test, X_general_test, y_test, stds_al, mu=0.95, eta=50):\n",
    "    predictions = np.array([model.predict([X_vector_test, X_scalar_test, X_general_test]) for model in ensemble])\n",
    "    means = np.mean(predictions, axis=0)\n",
    "    stds_ep = np.std(predictions, axis=0)\n",
    "    \n",
    "    lower_bounds_ep = means - mu * stds_ep\n",
    "    upper_bounds_ep = means + mu * stds_ep\n",
    "    \n",
    "    lower_bounds_al = means - mu * stds_al\n",
    "    upper_bounds_al = means + mu * stds_al\n",
    "    \n",
    "    lower_bounds_total = means - mu * (stds_ep + stds_al)\n",
    "    upper_bounds_total = means + mu * (stds_ep + stds_al)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    covered_ep = np.sum((y_test[:, 0] >= lower_bounds_ep[:, 0]) & (y_test[:, 0] <= upper_bounds_ep[:, 0]))\n",
    "    covered_al = np.sum((y_test[:, 0] >= lower_bounds_al[:, 0]) & (y_test[:, 0] <= upper_bounds_al[:, 0]))\n",
    "    covered_total = np.sum((y_test[:, 0] >= lower_bounds_total[:, 0]) & (y_test[:, 0] <= upper_bounds_total[:, 0]))\n",
    "    \n",
    "    picp_ep = covered_ep / len(y_test)\n",
    "    picp_al = covered_al / len(y_test)\n",
    "    picp_total = covered_total / len(y_test)\n",
    "    \n",
    "    mpiw_ep = np.mean(upper_bounds_ep[:, 0] - lower_bounds_ep[:, 0])\n",
    "    mpiw_al = np.mean(upper_bounds_al[:, 0] - lower_bounds_al[:, 0])\n",
    "    mpiw_total = np.mean(upper_bounds_total[:, 0] - lower_bounds_total[:, 0])\n",
    "    \n",
    "    target_range = np.max(y_test[:, 0]) - np.min(y_test[:, 0])\n",
    "    \n",
    "    \n",
    "    nmpiw_ep = mpiw_ep / target_range\n",
    "    nmpiw_al = mpiw_al / target_range\n",
    "    nmpiw_total = mpiw_total / target_range\n",
    "    \n",
    "    \n",
    "    mae = mean_absolute_error(y_test[:, 0], means[:, 0])\n",
    "    maep = mean_absolute_percentage_error(y_test[:, 0], means[:, 0])\n",
    "\n",
    "    results = {\n",
    "        'picp': {\n",
    "            'epistemic': picp_ep,\n",
    "            'aleatoric': picp_al,\n",
    "            'total': picp_total\n",
    "        },\n",
    "        'mpiw': {\n",
    "            'epistemic': mpiw_ep,\n",
    "            'aleatoric': mpiw_al,\n",
    "            'total': mpiw_total\n",
    "        },\n",
    "        'nmpiw': {\n",
    "            'epistemic': nmpiw_ep,\n",
    "            'aleatoric': nmpiw_al,\n",
    "            'total': nmpiw_total\n",
    "        },\n",
    "        'mae': mae,\n",
    "        'maep': maep\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tunining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_epochs = [10, 20, 30]\n",
    "transfer_epochs = [5, 10, 25]\n",
    "num_lstm_layers = [1, 2]\n",
    "num_dense_layers = [2, 3, 4]\n",
    "transfer_learning_rate = [0.0005, 0.001, 0.00025]\n",
    "n_members = [5, 10, 15]\n",
    "transfer_beta = [True, False]\n",
    "resampling_frequency = ['monthly', 'weekly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "New Iteration Start\n",
      "==================================\n",
      "41/41 [==============================] - 16s 276ms/step\n",
      "11/11 [==============================] - 6s 529ms/step\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    \n",
    "    print(\"==================================\")\n",
    "    print(\"New Iteration Start\")\n",
    "    print(\"==================================\")\n",
    "    \n",
    "    with open(f'../../data/{random.choice(resampling_frequency)}_resampling/X_scalar.pkl', 'rb') as f:\n",
    "        X_scalar = pickle.load(f)\n",
    "    \n",
    "    with open(f'../../data/{random.choice(resampling_frequency)}_resampling/X_vector.pkl', 'rb') as f:\n",
    "        X_vector = pickle.load(f)\n",
    "        \n",
    "    max_seq_len_vector = X_vector.shape[1]\n",
    "    dimension_vector = X_vector.shape[2]\n",
    "\n",
    "    max_seq_len_scalar = X_scalar.shape[1]\n",
    "    dimension_scalar = X_scalar.shape[2]\n",
    "    \n",
    "    train_size = int(0.8 * len(X_general))\n",
    "\n",
    "    X_vector_train, X_vector_test = X_vector[:train_size], X_vector[train_size:]\n",
    "    X_scalar_train, X_scalar_test = X_scalar[:train_size], X_scalar[train_size:]\n",
    "    X_general_train, X_general_test = X_general[:train_size], X_general[train_size:]\n",
    "    \n",
    "    \n",
    "    model_hyperparameter = {\n",
    "        \"lstm_units\": 128, \n",
    "        \"dense_units\": 128, \n",
    "        \"num_lstm_layers\": random.choice(num_lstm_layers), \n",
    "        \"num_dense_hidden_layers\": random.choice(num_dense_layers),\n",
    "        \"transfer_learning_rate\": random.choice(transfer_learning_rate),\n",
    "        \"activation\": 'relu',\n",
    "    }\n",
    "\n",
    "    train_hyperparameter = {\n",
    "        \"normal_epochs\": random.choice(normal_epochs),\n",
    "        \"transfer_epochs\": random.choice(transfer_epochs),\n",
    "        \"batch_size\": 32\n",
    "    }\n",
    "\n",
    "    n_members = random.choice(n_members)\n",
    "    transfer_beta = random.choice(transfer_beta)\n",
    "    \n",
    "    # Train model to predict SOH and append as feature\n",
    "    y = output[['OKT']].values\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    soh_model = create_model(max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, X_general_train.shape[1], SPECIAL_VALUE, **model_hyperparameter)\n",
    "\n",
    "    soh_model.fit([X_vector_train, X_scalar_train, X_general_train], y_train, epochs=train_hyperparameter['normal_epochs'], batch_size=train_hyperparameter['batch_size'], verbose=0)\n",
    "    \n",
    "    soh_pred_train = soh_model.predict([X_vector_train, X_scalar_train, X_general_train])\n",
    "    soh_pred_test = soh_model.predict([X_vector_test, X_scalar_test, X_general_test])\n",
    "    mae_soh = mean_absolute_error(y_test, soh_pred_test)\n",
    "    maep_soh = mean_absolute_percentage_error(y_test, soh_pred_test)\n",
    "\n",
    "    results = {}\n",
    "    results['SOH'] = {\n",
    "        'mae': mae_soh,\n",
    "        'maep': maep_soh\n",
    "    }\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    SOH_pred_train_norm = scaler.fit_transform(soh_pred_train)\n",
    "    SOH_pred_test_norm = scaler.transform(soh_pred_test)\n",
    "\n",
    "    X_general_train = np.hstack((X_general_train, SOH_pred_train_norm))\n",
    "    X_general_test = np.hstack((X_general_test, SOH_pred_test_norm))\n",
    "    \n",
    "    # Train ensemble for all parameters with transfer learning\n",
    "    ensemble = {}\n",
    "\n",
    "    parameters = ['alpha_PE', 'alpha_NE', 'beta_PE', 'beta_NE']\n",
    "\n",
    "    for i, parameter in enumerate(parameters):\n",
    "        y = output[[parameter]].values\n",
    "        y_train = y[:train_size]\n",
    "        pretrained_model = None\n",
    "        if parameter == 'alpha_NE':\n",
    "            pretrained_model = ensemble['alpha_PE'][0]\n",
    "        if parameter == 'beta_PE' and transfer_beta:\n",
    "            pretrained_model = ensemble['alpha_NE'][0]\n",
    "        if parameter == 'beta_NE':\n",
    "            pretrained_model = ensemble['beta_PE'][0]\n",
    "        ensemble[parameter] = fit_ensemble(n_members,\n",
    "                                            X_vector_train,\n",
    "                                            X_scalar_train, \n",
    "                                            X_general_train, \n",
    "                                            y_train,  \n",
    "                                            max_seq_len_vector, \n",
    "                                            dimension_vector, \n",
    "                                            max_seq_len_scalar, \n",
    "                                            dimension_scalar, \n",
    "                                            SPECIAL_VALUE,\n",
    "                                            model_hyperparameter,\n",
    "                                            train_hyperparameter,\n",
    "                                            parameter,\n",
    "                                            pretrained_model)\n",
    "    \n",
    "    # Train models to predict aleatoric uncertainty for each parameter\n",
    "    aleatoric_std = {}\n",
    "    for parameter in ['alpha_PE', 'alpha_NE', 'beta_PE', 'beta_NE']:\n",
    "        y = output[[parameter]].values\n",
    "        y_test = y[train_size:]\n",
    "        aleatoric_std[parameter] = estimate_aleatoric_uncertainty(\n",
    "            ensemble[parameter], \n",
    "            max_seq_len_vector,\n",
    "            dimension_vector,\n",
    "            max_seq_len_scalar,\n",
    "            dimension_scalar,\n",
    "            X_general_train.shape[1],\n",
    "            SPECIAL_VALUE,\n",
    "            X_vector_test, \n",
    "            X_scalar_test, \n",
    "            X_general_test, \n",
    "            y_test,\n",
    "            model_hyperparameter,\n",
    "            train_hyperparameter)\n",
    "        \n",
    "    # Evaluate ensemble for each parameter\n",
    "    for parameter in ['alpha_PE', 'alpha_NE', 'beta_PE', 'beta_NE']:\n",
    "        y = output[[parameter]].values\n",
    "        y_test = y[train_size:]\n",
    "        results[parameter] = evaluate_ensemble(ensemble[parameter], X_vector_test, X_scalar_test, X_general_test, y_test, aleatoric_std[parameter])\n",
    "        \n",
    "    # Save results\n",
    "    trial = {\n",
    "        'model_hyperparameter': model_hyperparameter,\n",
    "        'train_hyperparameter': train_hyperparameter,\n",
    "        'additional_hyperparameter': {\n",
    "            'n_members': n_members,\n",
    "            'transfer_beta': transfer_beta,\n",
    "            'resampling_frequency': resampling_frequency\n",
    "        },\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "    def convert_numpy_types(obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.int32):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "    # Load the existing JSON data from file\n",
    "    try:\n",
    "        with open('bootstrap_ensemble_results.json', 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    except FileNotFoundError:\n",
    "        data = []\n",
    "\n",
    "    # Append the new trial\n",
    "    data.append(trial)\n",
    "\n",
    "    # Save the updated data back to the JSON file\n",
    "    with open('bootstrap_ensemble_results.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4, default=convert_numpy_types)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
