{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, Input, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_VALUE = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = pd.read_csv('../../data/data_input_vector.csv')\n",
    "input_scalar = pd.read_csv('../../data/data_input_scalar.csv')\n",
    "input_general = pd.read_csv('../../data/data_input_general_Info.csv')\n",
    "output = pd.read_csv('../../data/data_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing of data_input_general_info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "# Get max sequence length\n",
    "max_seq_len_info = 1\n",
    "\n",
    "# Drop unneeded columns\n",
    "input_general = input_general.drop(columns=[\"VAN\", \"Index\", \"absolute_timestamp\", 'stat_zeitpunkt_test'], axis=1)\n",
    "\n",
    "# Replace 'BMW' with 'D'\n",
    "input_general['fzg_verkaufsland_kfzint_kez'] = input_general['fzg_verkaufsland_kfzint_kez'].replace('BMW', 'D')\n",
    "\n",
    "# Convert dates to timestamps\n",
    "input_general['fzg_produktionsdatum'] = pd.to_datetime(input_general['fzg_produktionsdatum']).astype('int64')\n",
    "\n",
    "# OMIT OUTLIER HANDLING\n",
    "\n",
    "# PREPROCESSING\n",
    "numerical_features = input_general.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = input_general.select_dtypes(include=['object']).columns\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "X_general = preprocessor.fit_transform(input_general)\n",
    "X_general = X_general.toarray()  # Convert sparse matrix to dense matrix\n",
    "\n",
    "dimension_info = X_general.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing of data_input_scalar.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before outlier removal: 167820\n",
      "Rows after outlier removal: 105645\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "# Drop unneeded columns\n",
    "input_scalar = input_scalar.drop(columns=[\"Unnamed: 0\", \"absolute_timestamp\"], axis=1)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "input_scalar = input_scalar.dropna()\n",
    "\n",
    "# Set auslesedatum relative to fzg_produktionsdatum (TODO check compatibility with resampling)\n",
    "#input_scalar['auslesedatum'] = pd.to_datetime(input_scalar['auslesedatum']).astype('int64')\n",
    "##input_general_copy = pd.read_csv('../../data/data_input_general_Info.csv')\n",
    "#input_general_copy['fzg_produktionsdatum'] = pd.to_datetime(input_general_copy['fzg_produktionsdatum']).astype('int64')\n",
    "#fzg_produktionsdatum_dict = input_general_copy.set_index('VAN')['fzg_produktionsdatum'].to_dict()\n",
    "#input_scalar['auslesedatum'] = input_scalar.apply(lambda row: row['auslesedatum'] - fzg_produktionsdatum_dict[row['VAN']], axis=1)\n",
    "\n",
    "# OUTLIER HANDLING\n",
    "\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=3.0):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_num = X.drop(columns=['auslesedatum'], axis=1).select_dtypes(include=['int64', 'float64'])\n",
    "        z_scores = np.abs((X_num - X_num.mean()) / X_num.std())\n",
    "        mask = (z_scores < self.threshold).all(axis=1)\n",
    "        \n",
    "        # Debug: Print the number of rows before and after outlier removal\n",
    "        print(\"Rows before outlier removal:\", X.shape[0])\n",
    "        print(\"Rows after outlier removal:\", X[mask].shape[0])\n",
    "        \n",
    "        return X[mask]\n",
    "    \n",
    "outlier_remover = OutlierRemover(threshold=1.96)\n",
    "input_scalar = outlier_remover.fit_transform(input_scalar)\n",
    "\n",
    "# RESAMPLING (currently disabled)\n",
    "\n",
    "# Convert dates to datetime\n",
    "input_scalar['auslesedatum'] = pd.to_datetime(input_scalar['auslesedatum'])\n",
    "\n",
    "# Sort values - this step is crucial for resampling\n",
    "input_scalar = input_scalar.sort_values(by=['VAN', 'auslesedatum'])\n",
    "\n",
    "# Group by 'VAN' and apply the resampling function\n",
    "#input_scalar = input_scalar.groupby('VAN').resample('ME', on='auslesedatum').mean().reset_index()\n",
    "\n",
    "# Convert dates to timestamps\n",
    "input_scalar['auslesedatum'] = pd.to_datetime(input_scalar['auslesedatum']).astype('int64')\n",
    "\n",
    "# PREPROCESSING\n",
    "\n",
    "numerical_features = input_scalar.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Sort by VAN and auslesedatum\n",
    "input_scalar = input_scalar.sort_values(by=['VAN', 'auslesedatum'])\n",
    "\n",
    "# Create 3D array with shape (num_vehicles, max_seq_len, dimension)\n",
    "num_vehicles = output['VAN'].nunique()\n",
    "max_seq_len_scalar = input_scalar.groupby('VAN').size().max()\n",
    "dimension_scalar = len(numerical_features)\n",
    "X_scalar = np.full((num_vehicles, max_seq_len_scalar, dimension_scalar), fill_value=SPECIAL_VALUE, dtype=np.float64)\n",
    "\n",
    "# Scaling + Encoding\n",
    "scaler = MinMaxScaler()\n",
    "input_scalar[numerical_features] = scaler.fit_transform(input_scalar[numerical_features])\n",
    "\n",
    "for van in input_scalar['VAN'].unique():\n",
    "    van_data = input_scalar[input_scalar['VAN'] == van]\n",
    "    input_sequence = van_data.drop(columns=['VAN']).values\n",
    "    index = output.index[output['VAN'] == van].to_list()[0]\n",
    "    seq_len = input_sequence.shape[0]\n",
    "    X_scalar[index, 0:seq_len, :] = input_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing of data_input_vector.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before outlier removal: 95274\n",
      "Rows after outlier removal: 74184\n"
     ]
    }
   ],
   "source": [
    "# Drop unneeded columns\n",
    "exclude_columns = [\"Unnamed: 0\", \"absolute_timestamp\", \"BAUREIHE\", \n",
    "                    \"stat_zeit_temp_total_12_wert\", \"stat_zeit_temp_total_13_wert\", \n",
    "                    \"stat_zeit_temp_total_14_wert\", \"stat_hv_spannung_wert\"]\n",
    "input_vector = input_vector.drop(columns=exclude_columns, axis=1)\n",
    "\n",
    "# Drop rows with NAN values\n",
    "input_vector = input_vector.dropna()\n",
    "\n",
    "# Convert dates to timestamps\n",
    "input_vector['fzg_produktionsdatum'] = pd.to_datetime(input_vector['fzg_produktionsdatum']).astype('int64')\n",
    "\n",
    "# Set auslesedatum relative to fzg_produktionsdatum (TODO check compatibility with resampling)\n",
    "#input_vector['auslesedatum'] = pd.to_datetime(input_vector['auslesedatum']).astype(int)\n",
    "#input_vector['auslesedatum'] = input_vector['auslesedatum'] - input_vector['fzg_produktionsdatum']\n",
    "\n",
    "# OUTLIER HANDLING\n",
    "\n",
    "class IQRBasedOutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor=1.5):\n",
    "        self.factor = factor\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_num = X.drop(columns=['auslesedatum'], axis=1).select_dtypes(include=['int64', 'float64'])\n",
    "        Q1 = X_num.quantile(0.25)\n",
    "        Q3 = X_num.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        mask = ~((X_num < (Q1 - self.factor * IQR)) | (X_num > (Q3 + self.factor * IQR))).any(axis=1)\n",
    "        \n",
    "        # Debug: Print the number of rows before and after outlier removal\n",
    "        print(\"Rows before outlier removal:\", X.shape[0])\n",
    "        print(\"Rows after outlier removal:\", X[mask].shape[0])\n",
    "        \n",
    "        return X[mask]\n",
    "\n",
    "outlier_remover = IQRBasedOutlierRemover(factor=20)\n",
    "input_vector = outlier_remover.fit_transform(input_vector)\n",
    "\n",
    "# RESAMPLING (currently disabled)\n",
    "\n",
    "# Convert dates to datetime\n",
    "input_vector['auslesedatum'] = pd.to_datetime(input_vector['auslesedatum'])\n",
    "\n",
    "# Sort values - this step is crucial for resampling\n",
    "input_vector = input_vector.sort_values(by=['VAN', 'auslesedatum'])\n",
    "\n",
    "# Group by 'VAN' and apply the resampling function\n",
    "#input_vector = input_vector.groupby('VAN').resample('ME', on='auslesedatum').mean().reset_index()\n",
    "\n",
    "# Convert dates to timestamps\n",
    "input_vector['auslesedatum'] = pd.to_datetime(input_vector['auslesedatum']).astype('int64')\n",
    "\n",
    "# PREPROCESSING\n",
    "\n",
    "numerical_features = input_vector.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Sort by VAN and auslesedatum\n",
    "input_vector = input_vector.sort_values(by=['VAN', 'auslesedatum'])\n",
    "\n",
    "# Create 3D array with shape (num_vehicles, max_seq_len, dimension)\n",
    "num_vehicles = output['VAN'].nunique()\n",
    "max_seq_len_vector = input_vector.groupby('VAN').size().max()\n",
    "dimension_vector = len(numerical_features)\n",
    "X_vector = np.full((num_vehicles, max_seq_len_vector, dimension_vector), fill_value=SPECIAL_VALUE, dtype=np.float64)\n",
    "\n",
    "# Scaling + Encoding\n",
    "scaler = MinMaxScaler()\n",
    "input_vector[numerical_features] = scaler.fit_transform(input_vector[numerical_features])\n",
    "\n",
    "for van in input_vector['VAN'].unique():\n",
    "    van_data = input_vector[input_vector['VAN'] == van]\n",
    "    input_sequence = van_data.drop(columns=['VAN']).values\n",
    "    index = output.index[output['VAN'] == van].to_list()[0]\n",
    "    seq_len = input_sequence.shape[0]\n",
    "    X_vector[index, 0:seq_len, :] = input_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(X_general))\n",
    "\n",
    "X_vector_train, X_vector_test = X_vector[:train_size], X_vector[train_size:]\n",
    "X_scalar_train, X_scalar_test = X_scalar[:train_size], X_scalar[train_size:]\n",
    "X_general_train, X_general_test = X_general[:train_size], X_general[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, eps=1e-6):\n",
    "    \"\"\"Gaussian negative log likelihood loss function.\"\"\"\n",
    "    mean = y_pred[:, 0]\n",
    "    log_var = y_pred[:, 1]\n",
    "    var = tf.maximum(tf.exp(log_var), eps)\n",
    "    loss = 0.5 * (tf.math.log(var) + tf.square(y_true - mean) / var)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def create_model(max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, max_seq_len_info, dimension_info,\n",
    "                 special_value, lstm_units=128, dense_units=128, num_lstm_layers=1, num_dense_hidden_layers=2, \n",
    "                 dropout_rate=0.4, transfer_learning_rate=0.001, activation='relu', transfer_learning=False):\n",
    "    input_layer_vector = Input(name=\"input_vector\", shape=(max_seq_len_vector, dimension_vector))\n",
    "    input_layer_scalar = Input(name=\"input_scalar\", shape=(max_seq_len_scalar, dimension_scalar))\n",
    "    input_layer_general = Input(name=\"input_general\", shape=(dimension_info,))\n",
    "\n",
    "    masked_vector = Masking(name=\"masking_input_vector\", mask_value=special_value)(input_layer_vector)\n",
    "    masked_scalar = Masking(name=\"masking_input_scalar\", mask_value=special_value)(input_layer_scalar)\n",
    "\n",
    "    # Create LSTM layers with the specified number of units and layers\n",
    "    lstm_output_vector = masked_vector\n",
    "    lstm_output_scalar = masked_scalar\n",
    "\n",
    "    for i in range(num_lstm_layers):\n",
    "        lstm_output_vector = LSTM(lstm_units, name=f\"lstm_vector_{i}\", return_sequences=True if i < num_lstm_layers - 1 else False)(lstm_output_vector)\n",
    "        lstm_output_scalar = LSTM(lstm_units, name=f\"lstm_scalar_{i}\", return_sequences=True if i < num_lstm_layers - 1 else False)(lstm_output_scalar)\n",
    "        if dropout_rate > 0:\n",
    "            lstm_output_vector = Dropout(dropout_rate, name=f\"dropout_vector_{i}\")(lstm_output_vector)\n",
    "            lstm_output_scalar = Dropout(dropout_rate, name=f\"dropout_scalar_{i}\")(lstm_output_scalar)\n",
    "\n",
    "    concat = concatenate([lstm_output_vector, lstm_output_scalar, input_layer_general])\n",
    "\n",
    "    # Create dense layers with the specified number of units and layers\n",
    "    dense_output = concat\n",
    "    for i in range(num_dense_hidden_layers):\n",
    "        dense_output = Dense(dense_units, activation=activation, name=f\"dense_{i}\")(dense_output)\n",
    "        if dropout_rate > 0:\n",
    "            dense_output = Dropout(dropout_rate, name=f\"dropout_dense_{i}\")(dense_output)\n",
    "\n",
    "    mean_output = Dense(1, name=\"mean\", activation=activation)(dense_output)\n",
    "    log_var_output = Dense(1, name=\"log_var\")(dense_output)\n",
    "    output = concatenate([mean_output, log_var_output])\n",
    "\n",
    "    model = Model(inputs=[input_layer_vector, input_layer_scalar, input_layer_general], outputs=output)\n",
    "    \n",
    "    optimizer = Adam(transfer_learning_rate if transfer_learning else 0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=custom_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_transfer_learning(base_model, max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, max_seq_len_info, dimension_info,\n",
    "                 special_value, lstm_units=128, dense_units=128, num_lstm_layers=1, num_dense_hidden_layers=2, \n",
    "                 dropout_rate=0.4, transfer_learning_rate=0.001, activation='relu'):\n",
    "    new_model = create_model(max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, max_seq_len_info, dimension_info,\n",
    "                             special_value, lstm_units, dense_units, num_lstm_layers, num_dense_hidden_layers, \n",
    "                             dropout_rate, transfer_learning_rate, activation, transfer_learning=True)\n",
    "    \n",
    "    # Load pretrained weights\n",
    "    for i in range(num_lstm_layers):\n",
    "        new_model.get_layer(name=f\"lstm_vector_{i}\").set_weights(base_model.get_layer(name=f\"lstm_vector_{i}\").get_weights())\n",
    "\n",
    "    for i in range(num_dense_hidden_layers):\n",
    "        new_model.get_layer(name=f\"dense_{i}\").set_weights(base_model.get_layer(name=f\"dense_{i}\").get_weights())\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hyperparameter = {\n",
    "    \"lstm_units\": 64, \n",
    "    \"dense_units\": 64, \n",
    "    \"num_lstm_layers\": 1, \n",
    "    \"num_dense_hidden_layers\": 2,\n",
    "    \"dropout_rate\": 0.4, \n",
    "    \"transfer_learning_rate\": 0.0005, \n",
    "    \"activation\": 'relu', \n",
    "}\n",
    "\n",
    "train_hyperparameter = {\n",
    "    \"normal_epochs\": 1,\n",
    "    \"transfer_epochs\": 1,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "transfer_beta = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Monte Carlo Dropout for all parameters with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 335ms/step - loss: -0.0904\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 329ms/step - loss: 0.1298\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 336ms/step - loss: -0.7166\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 473ms/step - loss: -0.5660\n"
     ]
    }
   ],
   "source": [
    "parameters = ['alpha_PE', 'alpha_NE', 'beta_PE', 'beta_NE']\n",
    "\n",
    "models = dict()\n",
    "\n",
    "for i, parameter in enumerate(parameters):\n",
    "    y = output[[parameter]].values\n",
    "    y_train = y[:train_size]\n",
    "    pretrained_model = None\n",
    "    if parameter == 'alpha_NE':\n",
    "        pretrained_model = models['alpha_PE']\n",
    "    if parameter == 'beta_PE' and transfer_beta:\n",
    "        pretrained_model = models['alpha_NE']\n",
    "    if parameter == 'beta_NE':\n",
    "        pretrained_model = models['beta_PE']\n",
    "    \n",
    "    if pretrained_model is not None:\n",
    "        model = create_model_transfer_learning(pretrained_model, max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, max_seq_len_info, dimension_info, SPECIAL_VALUE, **model_hyperparameter)\n",
    "    else:\n",
    "        model = create_model(max_seq_len_vector, dimension_vector, max_seq_len_scalar, dimension_scalar, max_seq_len_info, dimension_info, SPECIAL_VALUE, **model_hyperparameter)\n",
    "\n",
    "    model.fit(\n",
    "        [X_vector_train, X_scalar_train, X_general_train],\n",
    "        y_train,\n",
    "        epochs=train_hyperparameter['transfer_epochs'] if pretrained_model is not None else train_hyperparameter['normal_epochs'], \n",
    "        batch_size=train_hyperparameter['batch_size'],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    models[parameter] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Monte Carlo Droput for all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_dropout(model, inputs, n_iter=100):\n",
    "    means = np.zeros((n_iter, len(inputs[0])))\n",
    "    log_variances = np.zeros((n_iter, len(inputs[0])))\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        preds = model(inputs, training=True)\n",
    "        means[i] = preds[:,0]\n",
    "        log_variances[i] = preds[:,1]\n",
    "        \n",
    "    return means, log_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_bound(lower_bound, upper_bound, min_val, max_val, z_score, std):\n",
    "    for i in range(len(lower_bound)):\n",
    "        if min_val < lower_bound[i] and max_val > upper_bound[i]:\n",
    "            continue  # Case 1\n",
    "        elif lower_bound[i] < min_val and max_val > upper_bound[i]:\n",
    "            lower_bound[i] = min_val  # Case 2\n",
    "        elif min_val < lower_bound[i] and upper_bound[i] > max_val:\n",
    "            upper_bound[i] = max_val  # Case 3\n",
    "        elif lower_bound[i] < min_val and upper_bound[i] > max_val:\n",
    "            lower_bound[i] = min_val\n",
    "            upper_bound[i] = max_val  # Case 4\n",
    "        elif lower_bound[i] > max_val:\n",
    "            upper_bound[i] = max_val\n",
    "            lower_bound[i] = max(min_val, max_val - z_score * std[i])  # Case 5\n",
    "        elif upper_bound[i] < min_val:\n",
    "            lower_bound[i] = min_val\n",
    "            upper_bound[i] = min(max_val, min_val + z_score * std[i])  # Case 6\n",
    "            \n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mcd(model, X_vector_test, X_scalar_test, X_general_test, y_test, z_score=1.96, n_iter=2, aleatoric=True):\n",
    "    results = {}\n",
    "    \n",
    "    # Get predictions with dropout\n",
    "    means, log_variances = predict_with_dropout(model, [X_vector_test, X_scalar_test, X_general_test], n_iter=n_iter)\n",
    "    \n",
    "    max_val = np.max(y_test)\n",
    "    min_val = np.min(y_test)\n",
    "\n",
    "    # Calculate mean and variance according to the provided formula\n",
    "    pred_mean = means.mean(axis=0)\n",
    "\n",
    "    pred_epistemic_var = means.var(axis=0)\n",
    "    pred_aleatoric_var = np.exp(log_variances).mean(axis=0)\n",
    "    total_variance = pred_epistemic_var + (pred_aleatoric_var if aleatoric else 0)\n",
    "    pred_std = np.sqrt(total_variance)\n",
    "    pred_std_epistemic = np.sqrt(pred_epistemic_var)\n",
    "    pred_std_aleatoric = np.sqrt(pred_aleatoric_var)\n",
    "    \n",
    "    lower_bound = pred_mean - z_score * pred_std\n",
    "    upper_bound = pred_mean + z_score * pred_std\n",
    "    \n",
    "    lower_bound_epistemic = pred_mean - z_score * pred_std_epistemic\n",
    "    upper_bound_epistemic = pred_mean + z_score * pred_std_epistemic\n",
    "    \n",
    "    lower_bound_aleatoric = pred_mean - z_score * pred_std_aleatoric\n",
    "    upper_bound_aleatoric = pred_mean + z_score * pred_std_aleatoric\n",
    "        \n",
    "    # Total (unadjusted)\n",
    "    covered = np.sum((y_test.flatten() >= lower_bound.flatten()) & (y_test.flatten() <= upper_bound.flatten()))\n",
    "    picp = covered / len(y_test)\n",
    "    mpiw = np.mean(np.abs(upper_bound - lower_bound), axis=0)\n",
    "    target_range = max_val - min_val\n",
    "    nmpiw = mpiw / target_range\n",
    "    \n",
    "    lower_bound_adj, upper_bound_adj = adjust_bound(lower_bound, upper_bound, min_val, max_val, z_score, pred_std)\n",
    "    \n",
    "    # Total (adjusted)\n",
    "    covered_adj = np.sum((y_test.flatten() >= lower_bound_adj.flatten()) & (y_test.flatten() <= upper_bound_adj.flatten()))\n",
    "    picp_adj = covered_adj / len(y_test)\n",
    "    mpiw_adj = np.mean(upper_bound_adj - lower_bound_adj, axis=0)\n",
    "    nmpiw_adj = mpiw_adj / target_range\n",
    "    \n",
    "    # Epistemic\n",
    "    covered_epistemic = np.sum((y_test.flatten() >= lower_bound_epistemic.flatten()) & (y_test.flatten() <= upper_bound_epistemic.flatten()))\n",
    "    picp_epistemic = covered_epistemic / len(y_test)\n",
    "    mpiw_epistemic = np.mean(upper_bound_epistemic - lower_bound_epistemic, axis=0)\n",
    "    nmpiw_epistemic = mpiw_epistemic / target_range\n",
    "    \n",
    "    # Aleatoric\n",
    "    covered_aleatoric = np.sum((y_test.flatten() >= lower_bound_aleatoric.flatten()) & (y_test.flatten() <= upper_bound_aleatoric.flatten()))\n",
    "    picp_aleatoric = covered_aleatoric / len(y_test)\n",
    "    mpiw_aleatoric = np.mean(upper_bound_aleatoric - lower_bound_aleatoric, axis=0)\n",
    "    nmpiw_aleatoric = mpiw_aleatoric / target_range\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, pred_mean)\n",
    "    mape = mean_absolute_percentage_error(y_test, pred_mean)\n",
    "\n",
    "    results = {\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'picp': {\n",
    "            'epistemic': picp_epistemic,\n",
    "            'aleatoric': picp_aleatoric,\n",
    "            'total': picp,\n",
    "            'total_adj': picp_adj\n",
    "        },\n",
    "        'mpiw': {\n",
    "            'epistemic': mpiw_epistemic,\n",
    "            'aleatoric': mpiw_aleatoric,\n",
    "            'total': mpiw,\n",
    "            'total_adj': mpiw_adj\n",
    "        },\n",
    "        'nmpiw': {\n",
    "            'epistemic': nmpiw_epistemic,\n",
    "            'aleatoric': nmpiw_aleatoric,\n",
    "            'total': nmpiw,\n",
    "            'total_adj': nmpiw_adj,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score=1.96\n",
    "n_iter=2\n",
    "aleatoric=True\n",
    "\n",
    "results = {}\n",
    "\n",
    "for parameter in ['alpha_PE', 'alpha_NE', 'beta_PE', 'beta_NE']:\n",
    "    y = output[[parameter]].values\n",
    "    train_size = int(0.8 * len(y))\n",
    "    y_test = y[train_size:]\n",
    "    model = models[parameter]\n",
    "    results[parameter] = evaluate_mcd(model=model, X_general_test=X_general_test, X_scalar_test=X_scalar_test, X_vector_test=X_vector_test, y_test=y_test, n_iter=n_iter, aleatoric=aleatoric, z_score=z_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = {\n",
    "    'model_hyperparameter': model_hyperparameter,\n",
    "    'train_hyperparameter': train_hyperparameter,\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.float32):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.int32):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "# Load the existing JSON data from file\n",
    "try:\n",
    "    with open('monte_carlo_dropout_results.json', 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "except FileNotFoundError:\n",
    "    data = []\n",
    "\n",
    "# Append the new trial\n",
    "data.append(trial)\n",
    "\n",
    "# Save the updated data back to the JSON file\n",
    "with open('monte_carlo_dropout_results.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4, default=convert_numpy_types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
